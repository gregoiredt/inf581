{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KENDAMA RL PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulalisation of the environment\n",
    "\n",
    "If you execute the cell below, you will discover how the Kendama environment works. You can even change the action you want to give to the algorithm by modify the array action. You can also directly drag the ken and the dama on the simulation with your mouse. Have fun !\n",
    "\n",
    "#### Be careful : You need to reload the entire notebook if you closed the simulation's window and yet want to execute the simulation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from kendama_env import KendamaEnv\n",
    "import numpy as np \n",
    "import time\n",
    "env = KendamaEnv()\n",
    "\n",
    "a = 0\n",
    "start_time = time.time()\n",
    "while 1:\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "\n",
    "    if elapsed_time > 1.0/240:\n",
    "        a += 0.005\n",
    "        start_time = time.time()\n",
    "        # Change this if you want to give actions to the ken\n",
    "        action = [0,0,0.7*np.sin(10*a),0,0,0]\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        ob, reward, done, _ = env.step(action)\n",
    "\n",
    "        if(done):\n",
    "            env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading, Training and Evaluating the agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PP02 agent\n",
    "This cell lets you load the PPO2 agent we trained (on 5 millions iterations number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from kendama_env import KendamaEnv\n",
    "import numpy as np \n",
    "import time\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2,SAC\n",
    "from stable_baselines.common.vec_env import VecNormalize\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def make_env(rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = KendamaEnv(render=(rank==0))\n",
    "        return env\n",
    "    \n",
    "    return _init\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Uncomment this and comment below if you want to use multiprocessing\n",
    "    #num_cpu = 4\n",
    "    #env = SubprocVecEnv([make_env( i) for i in range(num_cpu)])\n",
    "    #env = VecNormalize(env, norm_reward= False)\n",
    "    env = KendamaEnv(render=True)\n",
    "    env = DummyVecEnv([lambda:env])\n",
    "    \n",
    "    env = VecNormalize.load(\"saves/vec_normalize_ppo.pkl\", env)\n",
    "    env.norm_reward = False\n",
    "\n",
    "    #model = PPO2(MlpPolicy, env, verbose=0,tensorboard_log=\"./log_model/\", gamma=0.985)\n",
    "    model = PPO2.load(\"saves/ppo2.zip\",env, verbose=0,tensorboard_log=\"./log_model2/\", gamma=0.985)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell allows you to train your agent on a certain number of timesteps. This will also execute a accelerated visualisation of your agent performance. To created your own agent, just remove the last # in the cell above and add them to the two loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model.learn(total_timesteps=20000000,tb_log_name=\"lavictoiredesesgrandsmorts\", reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell allows you to observe how your agent is behaving in real time. There is no more learning is this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "start_time = time.time()\n",
    "while 1:\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    if elapsed_time > 1.0/120:\n",
    "        start_time=current_time\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        if dones[0]:\n",
    "            obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell allows you to evaluate the performance of the loaded PPO algorithm (or the one you created) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "start_time = time.time()\n",
    "n_step = 1000\n",
    "\n",
    "catch = [0]\n",
    "t = [0]\n",
    "box = [0]\n",
    "\n",
    "rewards = n_step*[0]\n",
    "vmax = n_step*[0]\n",
    "vavg = n_step*[0]\n",
    "dminf = n_step*[0]\n",
    "davgf = n_step*[0]\n",
    "wavgf = n_step*[0]\n",
    "i = 0\n",
    "\n",
    "# Performance indicators :\n",
    "reward = [] # \n",
    "vmaxd = [0] #\n",
    "vavgd = [] #\n",
    "dmin = [1000000] \n",
    "davg = []\n",
    "wavgd = []\n",
    "first=True\n",
    "while 1:\n",
    "    if i == n_step:\n",
    "        break\n",
    "\n",
    "    \n",
    "    action, _states = model.predict(obs)\n",
    "    obs, _, dones, _ = env.env_method(\"evaluate\",action.reshape((6,)), reward,vmaxd,vavgd,dmin,davg,wavgd,catch,t,box)[0]\n",
    "    \n",
    "    if dones:\n",
    "        obs = env.reset()\n",
    "        rewards[i] = np.mean(reward)\n",
    "        vmax[i] = vmaxd[0]\n",
    "        vavg[i] = np.mean(vavgd)\n",
    "        dminf[i] = dmin[0]\n",
    "        davgf[i] = np.mean(davg)\n",
    "        wavgf[i] = np.mean(wavgd)\n",
    "        \n",
    "        reward = []\n",
    "        vmaxd = [0]\n",
    "        vavgd = []\n",
    "        dmin = [1000000]\n",
    "        davg = []\n",
    "        wavgd = []\n",
    "\n",
    "        i += 1\n",
    "        if not i%10:\n",
    "            clear_output(wait=True)\n",
    "            print(i//10,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC agent\n",
    "This cell lets you load the SAC agent we trained (on 5 millions iterations number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from kendama_env import KendamaEnv\n",
    "import numpy as np \n",
    "import time\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2,SAC\n",
    "from stable_baselines.common.vec_env import VecNormalize\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def make_env(rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = KendamaEnv(render=(rank==0))\n",
    "        return env\n",
    "    \n",
    "    return _init\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Uncomment this and comment below if you want to use multiprocessing\n",
    "    # num_cpu = 4\n",
    "    # env = SubprocVecEnv([make_env( i) for i in range(num_cpu)])\n",
    "    # env = VecNormalize(env, norm_reward= False)\n",
    "    env = KendamaEnv(render=True)\n",
    "    env = DummyVecEnv([lambda:env])\n",
    "    \n",
    "    env = VecNormalize.load(\"saves/vec_normalize_sac.pkl\", env)\n",
    "    env.norm_reward = False\n",
    "\n",
    "    #model = SAC(MlpPolicy, env, verbose=0,tensorboard_log=\"./log_model/\", gamma=0.985)\n",
    "    model = SAC.load(\"saves/sac.zip\",env, verbose=0,tensorboard_log=\"./log_model2/\", gamma=0.985)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell allows you to train your agent on a certain number of timesteps. This will also execute a accelerated visualisation of your agent performance. To created your own agent, just remove the last # in the cell above and add them to the two loading functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model.learn(total_timesteps=20000000,tb_log_name=\"lavictoiredesesgrandsmorts\", reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell allows you to observe how your agent is behaving in real time. There is no more learning is this phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "start_time = time.time()\n",
    "while 1:\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    if elapsed_time > 1.0/120:\n",
    "        start_time=current_time\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "        if dones[0]:\n",
    "            obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell allows you to evaluate the performance of the loaded SAC algorithm (or the one you created) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "start_time = time.time()\n",
    "n_step = 1000\n",
    "\n",
    "catch = [0]\n",
    "t = [0]\n",
    "box = [0]\n",
    "\n",
    "rewards = n_step*[0]\n",
    "vmax = n_step*[0]\n",
    "vavg = n_step*[0]\n",
    "dminf = n_step*[0]\n",
    "davgf = n_step*[0]\n",
    "wavgf = n_step*[0]\n",
    "i = 0\n",
    "\n",
    "# Performance indicators :\n",
    "reward = []\n",
    "vmaxd = [0]\n",
    "vavgd = []\n",
    "dmin = [1000000]\n",
    "davg = []\n",
    "wavgd = []\n",
    "first=True\n",
    "while 1:\n",
    "    if i == n_step:\n",
    "        break\n",
    "\n",
    "    \n",
    "    action, _states = model.predict(obs)\n",
    "    obs, _, dones, _ = env.env_method(\"evaluate\",action.reshape((6,)), reward,vmaxd,vavgd,dmin,davg,wavgd,catch,t,box)[0]\n",
    "    \n",
    "    if dones:\n",
    "        obs = env.reset()\n",
    "        rewards[i] = np.mean(reward)\n",
    "        vmax[i] = vmaxd[0]\n",
    "        vavg[i] = np.mean(vavgd)\n",
    "        dminf[i] = dmin[0]\n",
    "        davgf[i] = np.mean(davg)\n",
    "        wavgf[i] = np.mean(wavgd)\n",
    "        \n",
    "        reward = []\n",
    "        vmaxd = [0]\n",
    "        vavgd = []\n",
    "        dmin = [1000000]\n",
    "        davg = []\n",
    "        wavgd = []\n",
    "\n",
    "        i += 1\n",
    "        if not i%10:\n",
    "            clear_output(wait=True)\n",
    "            print(i//10,\"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
